{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e125dd2",
   "metadata": {},
   "source": [
    "# Importing model and necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73d50eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "370465ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path ='C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/detect.tflite'\n",
    "label_path='C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/labelmap.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6c39aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tensorflow Lite model into memory\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "input_mean = 127.5\n",
    "input_std = 127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38cc57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import importlib.util\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acd356",
   "metadata": {},
   "source": [
    "# Defining a function to test on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67588117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import importlib.util\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define function for object detection using a TensorFlow Lite model and displaying results\n",
    "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/path', txt_only=False):\n",
    "    \"\"\"\n",
    "    Perform object detection using a TensorFlow Lite model on test images.\n",
    "\n",
    "    Parameters:\n",
    "        modelpath (str): Path to the TensorFlow Lite model file.\n",
    "        imgpath (str): Path to the folder containing test images.\n",
    "        lblpath (str): Path to the label map file.\n",
    "        min_conf (float, optional): Minimum confidence threshold for displaying detections. Default is 0.5.\n",
    "        num_test_images (int, optional): Number of test images to randomly select for inference. Default is 10.\n",
    "        savepath (str, optional): Path to the folder where the text files will be saved. Default is '/content/results'.\n",
    "        txt_only (bool, optional): Flag to control whether to display images with results or just save the results in text files. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab filenames of all images in test folder\n",
    "    images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
    "\n",
    "    # Load the label map into memory\n",
    "    with open(lblpath, 'r') as f:\n",
    "        labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Load the Tensorflow Lite model into memory\n",
    "    interpreter = tf.lite.Interpreter(model_path=modelpath)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get model details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "    input_mean = 127.5\n",
    "    input_std = 127.5\n",
    "\n",
    "    # Randomly select test images\n",
    "    images_to_test = random.sample(images, num_test_images)\n",
    "\n",
    "    # Loop over every image and perform detection\n",
    "    for image_path in images_to_test:\n",
    "\n",
    "        # Load image and resize to expected shape [1xHxWx3]\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        imH, imW, _ = image.shape\n",
    "        image_resized = cv2.resize(image_rgb, (width, height))\n",
    "        input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "        # Normalize pixel values if using a floating model (i.e., if model is non-quantized)\n",
    "        if float_input:\n",
    "            input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "        # Perform the actual detection by running the model with the image as input\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Retrieve detection results\n",
    "        boxes = interpreter.get_tensor(output_details[1]['index'])[0]  # Bounding box coordinates of detected objects\n",
    "        classes = interpreter.get_tensor(output_details[3]['index'])[0]  # Class index of detected objects\n",
    "        scores = interpreter.get_tensor(output_details[0]['index'])[0]  # Confidence of detected objects\n",
    "\n",
    "        detections = []\n",
    "\n",
    "        # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "        for i in range(len(scores)):\n",
    "            if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "\n",
    "                # Get bounding box coordinates and draw box\n",
    "                # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "                ymin = int(max(1, (boxes[i][0] * imH)))\n",
    "                xmin = int(max(1, (boxes[i][1] * imW)))\n",
    "                ymax = int(min(imH, (boxes[i][2] * imH)))\n",
    "                xmax = int(min(imW, (boxes[i][3] * imW)))\n",
    "\n",
    "                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n",
    "\n",
    "                # Draw label\n",
    "                object_name = labels[int(classes[i])]  # Look up object name from \"labels\" array using class index\n",
    "                label = '%s: %d%%' % (object_name, int(scores[i] * 100))  # Example: 'person: 72%'\n",
    "                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)  # Get font size\n",
    "                label_ymin = max(ymin, labelSize[1] + 10)  # Make sure not to draw label too close to the top of the window\n",
    "                cv2.rectangle(image, (xmin, label_ymin - labelSize[1] - 10), (xmin + labelSize[0], label_ymin + baseLine - 10), (255, 255, 255), cv2.FILLED)  # Draw white box to put label text in\n",
    "                cv2.putText(image, label, (xmin, label_ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)  # Draw label text\n",
    "\n",
    "                detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # All the results have been drawn on the image, now display the image or save detection results in .txt files (for calculating mAP)\n",
    "        if not txt_only:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(12, 16))\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Get filenames and paths\n",
    "            image_fn = os.path.basename(image_path)\n",
    "            base_fn, ext = os.path.splitext(image_fn)\n",
    "            txt_result_fn = base_fn + '.txt'\n",
    "            txt_savepath = os.path.join(savepath, txt_result_fn)\n",
    "\n",
    "            # Write results to text file\n",
    "            # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
    "            with open(txt_savepath, 'w') as f:\n",
    "                for detection in detections:\n",
    "                  f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db2bad",
   "metadata": {},
   "source": [
    "# Defining a function to run inference with webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b9add2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.8.0.74-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from opencv-python) (1.21.6)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.0.74\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --user opencv-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcd7d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL.Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def tflite_detect_camera(modelpath, lblpath, min_conf=0.5):\n",
    "    \"\"\"\n",
    "    Perform real-time object detection using a TensorFlow Lite model with the webcam.\n",
    "\n",
    "    Parameters:\n",
    "        modelpath (str): Path to the TensorFlow Lite model file.\n",
    "        lblpath (str): Path to the label map file.\n",
    "        min_conf (float, optional): Minimum confidence threshold for displaying detections. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the label map into memory\n",
    "    with open(lblpath, 'r') as f:\n",
    "        labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Load the TensorFlow Lite model into memory\n",
    "    interpreter = tf.lite.Interpreter(model_path=modelpath)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get model details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "    input_mean = 127.5\n",
    "    input_std = 127.5\n",
    "\n",
    "    # Initialize webcam stream\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for the default camera or a specific camera index\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Capture a frame from the webcam\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Check if the frame is captured successfully\n",
    "            if not ret:\n",
    "                print(\"Error: Could not capture frame.\")\n",
    "                break\n",
    "\n",
    "            # Preprocess the input frame (resize and normalize)\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_resized = cv2.resize(image_rgb, (width, height))\n",
    "            input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "            # Normalize pixel values if using a floating model (i.e., if model is non-quantized)\n",
    "            if float_input:\n",
    "                input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "            # Perform object detection by running the model with the image as input\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "            interpreter.invoke()\n",
    "\n",
    "            # Retrieve detection results\n",
    "            boxes = interpreter.get_tensor(output_details[1]['index'])[0]  # Bounding box coordinates of detected objects\n",
    "            classes = interpreter.get_tensor(output_details[3]['index'])[0]  # Class index of detected objects\n",
    "            scores = interpreter.get_tensor(output_details[0]['index'])[0]  # Confidence of detected objects\n",
    "\n",
    "            # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "            for i in range(len(scores)):\n",
    "                if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "                    # Get bounding box coordinates and draw box\n",
    "                    ymin = int(max(1, (boxes[i][0] * frame.shape[0])))\n",
    "                    xmin = int(max(1, (boxes[i][1] * frame.shape[1])))\n",
    "                    ymax = int(min(frame.shape[0], (boxes[i][2] * frame.shape[0])))\n",
    "                    xmax = int(min(frame.shape[1], (boxes[i][3] * frame.shape[1])))\n",
    "\n",
    "                    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n",
    "\n",
    "                    # Draw label\n",
    "                    object_name = labels[int(classes[i])]\n",
    "                    label = '%s: %.2f' % (object_name, scores[i])\n",
    "                    cv2.putText(frame, label, (xmin, ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Convert the frame to RGB and display it using IPython.display\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = PIL.Image.fromarray(frame_rgb)\n",
    "            display(pil_image)\n",
    "\n",
    "            # Clear the previous output to show the updated frame\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Press 'Ctrl+C' to stop the webcam stream and close the window\n",
    "        print(\"Webcam stream stopped by user.\")\n",
    "\n",
    "    # Release the webcam and close the OpenCV window\n",
    "    cap.release()\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbb39515",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_detect_camera(model_path, label_path)\n",
    "#if you want to stop the stream, select the cell block, go to kernel then interrupt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa93684",
   "metadata": {},
   "source": [
    "# Defining a function that saves the detected images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912cb31",
   "metadata": {},
   "source": [
    "the following code adds cropping the ROI(region of interest) from the detected licence plates and saves them in a local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16e53d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL.Image\n",
    "import os\n",
    "\n",
    "def tflite_detect_camera_ROI(modelpath, lblpath, min_conf=0.5, save_folder=\"C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/Detected_Plates\"):\n",
    "    \"\"\"\n",
    "    Perform real-time object detection using a TensorFlow Lite model with the webcam.\n",
    "\n",
    "    Parameters:\n",
    "        modelpath (str): Path to the TensorFlow Lite model file.\n",
    "        lblpath (str): Path to the label map file.\n",
    "        min_conf (float, optional): Minimum confidence threshold for displaying detections. Default is 0.5.\n",
    "        save_folder (str, optional): Path to save the cropped license plate images. Default is \"detected_plates/\".\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the folder to save the detected license plate images\n",
    "    #os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Load the label map into memory\n",
    "    with open(lblpath, 'r') as f:\n",
    "        labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Load the TensorFlow Lite model into memory\n",
    "    interpreter = tf.lite.Interpreter(model_path=modelpath)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get model details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "    input_mean = 127.5\n",
    "    input_std = 127.5\n",
    "\n",
    "    # Initialize webcam stream\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for the default camera or a specific camera index\n",
    "\n",
    "    # Create a counter variable to ensure unique image names\n",
    "    counter = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Capture a frame from the webcam\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Check if the frame is captured successfully\n",
    "            if not ret:\n",
    "                print(\"Error: Could not capture frame.\")\n",
    "                break\n",
    "\n",
    "            # Preprocess the input frame (resize and normalize)\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_resized = cv2.resize(image_rgb, (width, height))\n",
    "            input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "            # Normalize pixel values if using a floating model (i.e., if model is non-quantized)\n",
    "            if float_input:\n",
    "                input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "            # Perform object detection by running the model with the image as input\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "            interpreter.invoke()\n",
    "\n",
    "            # Retrieve detection results\n",
    "            boxes = interpreter.get_tensor(output_details[1]['index'])[0]  # Bounding box coordinates of detected objects\n",
    "            classes = interpreter.get_tensor(output_details[3]['index'])[0]  # Class index of detected objects\n",
    "            scores = interpreter.get_tensor(output_details[0]['index'])[0]  # Confidence of detected objects\n",
    "\n",
    "            # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "            for i in range(len(scores)):\n",
    "                if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "                    # Get bounding box coordinates and draw box\n",
    "                    ymin = int(max(1, (boxes[i][0] * frame.shape[0])))\n",
    "                    xmin = int(max(1, (boxes[i][1] * frame.shape[1])))\n",
    "                    ymax = int(min(frame.shape[0], (boxes[i][2] * frame.shape[0])))\n",
    "                    xmax = int(min(frame.shape[1], (boxes[i][3] * frame.shape[1])))\n",
    "\n",
    "                    # Draw bounding box and label\n",
    "                    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n",
    "                    object_name = labels[int(classes[i])]\n",
    "                    label = '%s: %.2f' % (object_name, scores[i])\n",
    "                    cv2.putText(frame, label, (xmin, ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "                    # Crop the detected license plate region\n",
    "                    license_plate = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "                    # Save the cropped license plate as an image with a unique name using the counter\n",
    "                    image_name = f\"plate_{counter}.jpg\"\n",
    "                    counter += 1\n",
    "                    save_filepath = os.path.join(save_folder, image_name)\n",
    "                    cv2.imwrite(save_filepath, license_plate)\n",
    "\n",
    "            # Convert the frame to RGB and display it using IPython.display\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = PIL.Image.fromarray(frame_rgb)\n",
    "            display(pil_image)\n",
    "\n",
    "            # Clear the previous output to show the updated frame\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        # Press 'Ctrl+C' to stop the webcam stream and close the window\n",
    "        print(\"Webcam stream stopped by user.\")\n",
    "\n",
    "    # Release the webcam and close the OpenCV window\n",
    "    cap.release()\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e2712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these paths with your specific model and label map paths\n",
    "model_path = 'C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/detect.tflite'\n",
    "label_path = 'C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/labelmap.txt'\n",
    "\n",
    "# Set the minimum confidence threshold (you can adjust this value as needed)\n",
    "min_confidence = 0.5\n",
    "save_folder=\"C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/Detected_Plates\"\n",
    "\n",
    "# Call the function for real-time license plate detection and cropping\n",
    "tflite_detect_camera_ROI(model_path, label_path, min_confidence, save_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47261d0c",
   "metadata": {},
   "source": [
    "# Appliying OCR to extract number plates from the saved images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aba19cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (0.10.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.14.1-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\envs\\anpr\\lib\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision --upgrade --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccbfdd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092de30",
   "metadata": {},
   "source": [
    "Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e7af4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB |\n",
      "FT 898\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "def ocr_image(image_path, languages=['en']):\n",
    "    \"\"\"\n",
    "    Perform OCR on an image and extract text.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the input image.\n",
    "        languages (list, optional): List of languages to recognize. Default is ['en'] (English).\n",
    "\n",
    "    Returns:\n",
    "        List of text detections in the format: [(bbox, text, score), ...]\n",
    "    \"\"\"\n",
    "    # Initialize the EasyOCR reader\n",
    "    reader = easyocr.Reader(languages)\n",
    "\n",
    "    # Read text from the image\n",
    "    result = reader.readtext(image_path)\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'path/to/your/image.jpg' with the actual path to the image you want to process\n",
    "    image_path = \"C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/Detected_Plates/plate_0.jpg\"\n",
    "    \n",
    "    # Perform OCR on the image\n",
    "    detections = ocr_image(image_path)\n",
    "\n",
    "    # Display the result\n",
    "    for detection in detections:\n",
    "        text = detection[1]\n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173911a5",
   "metadata": {},
   "source": [
    "The following code defines a function that loops over the saved images, apply OCR on each image and saves the results in a csv file. The resulting csv file contains 2 columns: image name and detected licence plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f12c482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR results saved to ocr_results.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import easyocr\n",
    "\n",
    "def ocr_and_save_results(folder_path, output_csv=\"ocr_results.csv\", languages=['en']):\n",
    "    \"\"\"\n",
    "    Perform OCR on images in a folder and save the results to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): Path to the folder containing the detected plates.\n",
    "        output_csv (str, optional): Path to save the CSV file. Default is \"ocr_results.csv\".\n",
    "        languages (list, optional): List of languages to recognize. Default is ['en'] (English).\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Initialize the EasyOCR reader\n",
    "    reader = easyocr.Reader(languages)\n",
    "\n",
    "    # Create a CSV file to save the OCR results\n",
    "    csv_file = os.path.join(folder_path, output_csv)\n",
    "\n",
    "    # Open the CSV file in write mode and create a CSV writer\n",
    "    with open(csv_file, mode='w', newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "\n",
    "        # Write the header row in the CSV file\n",
    "        csv_writer.writerow([\"Image Name\", \"Extracted Plate Number\"])\n",
    "\n",
    "        # Loop through the files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # Check if the file is an image (you can add more checks if needed)\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
    "                # Construct the full path of the image\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                # Perform OCR on the image\n",
    "                detections = reader.readtext(image_path)\n",
    "\n",
    "                # Extract the plate numbers from OCR results\n",
    "                plate_numbers = [detection[1] for detection in detections]\n",
    "\n",
    "                # Write the image name and extracted plate numbers to the CSV file\n",
    "                csv_writer.writerow([filename, ', '.join(plate_numbers)])\n",
    "\n",
    "    print(f\"OCR results saved to {output_csv}.\")\n",
    "\n",
    "# Now you can call the function with the folder path and optional parameters\n",
    "folder_path = \"C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/Detected_Plates\"\n",
    "ocr_and_save_results(folder_path, output_csv=\"ocr_results.csv\", languages=['en'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c247b",
   "metadata": {},
   "source": [
    "Here, we establish a function aimed at refining our recurrent license plate detections, particularly when encountering multiple instances of the same plate. This function identifies and removes rows with duplicate license plate numbers obtained through Optical Character Recognition (OCR). Simultaneously, it eliminates recurring images from the folder dedicated to detected plates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d798d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38d22a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent plates and images deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def delete_recurrent_plates_and_images(csv_file, folder_path):\n",
    "    \"\"\"\n",
    "    Delete rows with recurrent extracted plate numbers from the CSV file.\n",
    "    Also, delete all images with recurring image names from the folder.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (str): Path to the CSV file containing OCR results.\n",
    "        folder_path (str): Path to the folder containing the images.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Find recurrent extracted plate numbers\n",
    "    recurrent_plate_numbers = df[df.duplicated(subset=\"Extracted Plate Number\", keep=False)][\"Extracted Plate Number\"].unique()\n",
    "\n",
    "    # Loop through the recurrent plate numbers\n",
    "    for plate_number in recurrent_plate_numbers:\n",
    "        # Get the image names corresponding to the recurrent plate number\n",
    "        recurrent_images = df[df[\"Extracted Plate Number\"] == plate_number][\"Image Name\"].tolist()\n",
    "\n",
    "        # Loop through the images in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # Check if the file is an image and has a recurrent image name\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')) and filename in recurrent_images:\n",
    "                # Construct the full path of the image\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                # Delete the image\n",
    "                os.remove(image_path)\n",
    "\n",
    "        # Remove all rows with the recurrent extracted plate number from the DataFrame\n",
    "        df = df[df[\"Extracted Plate Number\"] != plate_number]\n",
    "\n",
    "    # Save the cleaned DataFrame back to the CSV file\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    print(\"Recurrent plates and images deleted successfully.\")\n",
    "\n",
    "# Now you can call the function with the CSV file and folder path\n",
    "csv_file_path = \"C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/Detected_Plates/ocr_results.csv\"\n",
    "folder_path = \"C:/Users/Dell/Desktop/Project/custom_model_lite/custom_model_lite/Detected_Plates\"\n",
    "delete_recurrent_plates_and_images(csv_file_path, folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13554a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANPR",
   "language": "python",
   "name": "anpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
